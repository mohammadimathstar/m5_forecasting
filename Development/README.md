# ğŸ›  Development: Model Training Pipeline

An end-to-end machine learning pipeline for training a sales forecasting model using the M5 dataset. Built with modular components using **LightGBM**, **Hyperopt**, **Prefect**, and **MLflow**, the pipeline supports data ingestion, feature engineering, hyperparameter optimization, and model training with custom evaluation metrics.


---


## ğŸ”„ Workflow

1. Load and preprocess data
2. Perform feature engineering
3. Run hyperparameter tuning with Hyperopt
4. Train a LightGBM model

---

## ğŸš€ Features

- âœ… Modular pipeline architecture using [Prefect](https://docs.prefect.io/)
- ğŸ” Full feature engineering: lag features, rolling stats, calendar joins
- ğŸ§ª Hyperparameter tuning via [Hyperopt](https://github.com/hyperopt/hyperopt)
- ğŸ“Š Custom metrics: sMAPE and MASE
- ğŸ“ Experiment tracking with [MLflow](https://mlflow.org/)
- ğŸ§¼ Testable, maintainable, and MLOps-ready
- ğŸ“¦ Lightweight with YAML-configured parameters

---

## ğŸ“‚ Structure

```
development/
â”œâ”€â”€ codes/
â”‚ â”œâ”€â”€ data_loader.py
â”‚ â”œâ”€â”€ feature_engineering.py
â”‚ â”œâ”€â”€ best_model.py
â”‚ â”œâ”€â”€ config.py
â”‚ â”œâ”€â”€ tuning/
â”‚ â”œâ”€â”€ metrics/
â”‚ â””â”€â”€ models/
â”œâ”€â”€ tests/                  # Unit tests
â”œâ”€â”€ data/
â”œâ”€â”€ pipeline_training.py
â”œâ”€â”€ params.yaml             # Pipeline configuration
â”œâ”€â”€ .env.example # Environment variables template
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile                # Workflow automation
â”œâ”€â”€ start_prefect.sh
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md â† this file
```

---

## âš™ï¸ Setup

### 1. Clone the repo

```bash
git clone https://github.com/mohammadimathstar/m5-forecasting.git
cd m5-forecasting/Development
```

### 2. Install dependencies

```bash
uv venv
uv pip install -r requirements.txt
```

### 3. Configure environment

```bash
cp .env.example .env
# Then edit `.env` with your credentials (for using S3)
```


### ğŸ“ Data

- Place the original `sales_train_validation.csv`, `calendar.csv`, and `sell_prices.csv` inside: `data/raw/`

- Processed features and `test/reference` sets will be saved in: `data/processed/`



## ğŸš€ Running the Pipeline (with make)

Follow these steps to run the M5 forecasting pipeline locally:

#### 1. Activate the virtual environment

```bash
source .venv/bin/activate
```

#### 2. Run the pipeline

You can run the pipeline using `run_pipeline.sh` 

```bash
chmod +x run_pipeline.sh
./run_pipeline.sh
```
It opens a menu. You can select `Store code on a local filesystem`.

----

### MLflow

To track the experiment, we use Mlflow. You can modify its configuration in `run_pipeline.sh` script:

- backend-store-uri: the location for storing the meta-data, such as metrics, hyperparameters and etc. Here, we use sqlite.

- default-artifact-root: the location for storing the final model with its artifacts. Here, we use AWS S3.

- host and port: to see the experiment runs (default is `localhost:5000`)

Note. You must create the S3 bucket before running the script.

![Mlflow](pics/mlflow-training.png)

----

### Prefect

To see the progress of the workflow, you can open:
```bash
localhost:4200
```
and click on `Runs` tab.


![Prefect](pics/prefect-training.png)



### ğŸ“¦ Makefile Commands

- `make install`  
  Installs all required Python dependencies listed in `requirements.txt`.

- `make test`  
  Runs all unit tests using `pytest` to verify code correctness.

- `make lint`  
  Checks code style and quality using `ruff`, `black`, and `isort`.  
  *Note:* This command **checks** formatting and reports issues but does **not** modify the code.

- `make lint-fix`  
  Automatically fixes linting and formatting issues with `ruff`.

- `make format`  
  Formats code using `ruff`.

- `make run`
  To run the pipeline.

- `make clean`  
  Removes Python bytecode files (`__pycache__`, `.pyc`, `.pyo`) to keep the repo clean.


