# ğŸš€ Deployment: Batch Inference & Monitoring

This folder simulates a **production pipeline** where daily data is ingested, predictions are made using the trained model, and the model is monitored for drift and performance degradation.


---


## ğŸ”„ Workflow

1. Load a trained model from MLflow or local disk
2. Simulate daily input data
3. Predict using the model
4. Log predictions and metrics
5. Monitor drift and update the dashboard


---

## ğŸš€ Features

- ğŸ“‰ **Drift detection** using Evidently (e.g., sMAPE, MASE, data stats)
- ğŸ›¢ **PostgreSQL** for storing monitoring metrics
- ğŸ” **Adminer** for database browsing
- ğŸ“Š **Grafana** for model monitoring dashboards
- âš™ï¸ **Prefect** to orchestrate the entire pipeline


---
## ğŸ“‚ Structure

Deployment/
â”œâ”€â”€ codes/
â”‚ â”œâ”€â”€ db.py
â”‚ â”œâ”€â”€ model.py
â”‚ â”œâ”€â”€ prediction.py
â”‚ â”œâ”€â”€ config.py
â”‚ â””â”€â”€ reporting.py
â”œâ”€â”€ config/
â”œâ”€â”€ dashboards/
â”œâ”€â”€ data/
â”œâ”€â”€ tests/
â”œâ”€â”€ monitoring_pipeline.py
â”œâ”€â”€ start_prefect_deployment.sh
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ params.yaml             # Pipeline configuration
â”œâ”€â”€ .env.example            # Environment variables template (for AWS S3)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile                # Workflow automation
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md â† this file



---

## ğŸ”§ Setup Instructions

### 1. ğŸ“¦ Install Python Dependencies

Create a virtual environment and install dependencies:

```bash
make install
```

Or manually:

```bash
pip install -r requirements.txt
```

### 2. ğŸŒ Environment Setup

Copy the .env.example and populate it:

```bash
cp .env.example .env
```

Update the following:

- `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` for S3 (MLflow)

Check `params.yaml`, and modify it accordingly.

### 3. ğŸ³ Start Services via Docker

This brings up:

- PostgreSQL

- Adminer (http://localhost:8080)

- Grafana (http://localhost:3000)

```bash
docker-compose up -d
```

### 4. ğŸš€ Start Deployment Pipeline
Start Prefect + Worker + Deploy:

```bash
bash start_prefect_deployment.sh
```

This script:

- Launches Prefect server

- Registers your pipeline as a Prefect deployment

- Starts a Prefect worker

- Run Inference + Monitoring

## Workflow

After deployment:, it follows the following workflow:

- Loads new data for a day (daily)

- Makes predictions (for the given day)

- Calculates metrics and drift (via Evidently)

- Saves evaluation reports (in PostgreSQL)

- Update the dashboard with the reports (in Grafana)


## ğŸ“Š Visualization & Monitoring

### ğŸ“‰ Evidently

Drift metrics and distribution reports are automatically generated.

### ğŸ—ƒ Adminer
To explore your PostgreSQL database (containing drift metrics):

```arduino
http://localhost:8080
```

Login :

- Username: postgres

- Password: example

- Database: test

### ğŸ“ˆ Grafana

To view monitoring dashboards:

```arduino
http://localhost:3000
```

Login (default):

- Username: admin

- Password: admin



